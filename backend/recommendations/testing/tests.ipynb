{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrank_bm25\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BM25Okapi\n",
      "File \u001b[0;32m~/.pyenv/versions/trehaus-backend/lib/python3.12/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/trehaus-backend/lib/python3.12/site-packages/gensim/corpora/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/trehaus-backend/lib/python3.12/site-packages/gensim/corpora/indexedcorpus.py:14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[1;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[0;32m~/.pyenv/versions/trehaus-backend/lib/python3.12/site-packages/gensim/interfaces.py:19\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[1;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[0;32m~/.pyenv/versions/trehaus-backend/lib/python3.12/site-packages/gensim/matutils.py:1034\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(set1 \u001b[38;5;241m&\u001b[39m set2)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(union_cardinality)\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;66;03m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogsumexp\u001b[39m(x):\n",
      "File \u001b[0;32m~/.pyenv/versions/trehaus-backend/lib/python3.12/site-packages/gensim/_matutils.pyx:1\u001b[0m, in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.models import Word2Vec\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "     id                                       title  \\\n",
      "0     1                             E-Commerce Data   \n",
      "1    10  Sales of summer clothes in E-commerce Wish   \n",
      "2  1000         AV JanataHack Cross-Sell Prediction   \n",
      "3  1000                             E-Commerce Data   \n",
      "4  1000                             E-Commerce Data   \n",
      "\n",
      "                                         description  \\\n",
      "0               Actual transactions from UK retailer   \n",
      "1  Top products with ratings and sales performanc...   \n",
      "2                             Janata Hack Cross-Sell   \n",
      "3               Actual transactions from UK retailer   \n",
      "4               Actual transactions from UK retailer   \n",
      "\n",
      "                                                 url   size format  \\\n",
      "0      https://www.kaggle.com/carrie1/ecommerce-data    7MB    csv   \n",
      "1  https://www.kaggle.com/jmmvutu/summer-products...  406KB    csv   \n",
      "2  https://www.kaggle.com/jinxzed/av-janatahack-c...    6MB    csv   \n",
      "3      https://www.kaggle.com/carrie1/ecommerce-data    7MB    csv   \n",
      "4      https://www.kaggle.com/carrie1/ecommerce-data    7MB    csv   \n",
      "\n",
      "                                     tfidf_embedding  \\\n",
      "0  \\x00000000000000000000000000000000000000000000...   \n",
      "1  \\x00000000000000000000000000000000000000000000...   \n",
      "2  \\x00000000000000000000000000000000000000000000...   \n",
      "3  \\x00000000000000000000000000000000000000000000...   \n",
      "4  \\x00000000000000000000000000000000000000000000...   \n",
      "\n",
      "                                      bert_embedding  source  \\\n",
      "0  \\x9c22033d41e9b0bc0ad317bd2b8e56bdcd02cdbb7352...  Kaggle   \n",
      "1  \\xeaf32dbcc6da9f3cbfb747bc71eac73ca4a2713dd6bd...  Kaggle   \n",
      "2  \\x3efc07be92c1a7bd2323ccbcd2855ebcc9c71ebca960...  Kaggle   \n",
      "3  \\x3efc07be92c1a7bd2323ccbcd2855ebcc9c71ebca960...  Kaggle   \n",
      "4  \\x3efc07be92c1a7bd2323ccbcd2855ebcc9c71ebca960...  Kaggle   \n",
      "\n",
      "                   date_added  date_updated  \\\n",
      "0  2024-09-25 22:30:48.753641           NaN   \n",
      "1  2024-09-25 22:31:50.124325           NaN   \n",
      "2  2024-09-25 21:04:47.835632           NaN   \n",
      "3  2024-09-25 23:58:44.849257           NaN   \n",
      "4  2024-09-25 22:57:59.432061           NaN   \n",
      "\n",
      "                                           data_hash  \n",
      "0  27a33072ab08a07be2eb39e529345fe3ca3a2aca05967b...  \n",
      "1  d62eb13482b85e25268a00816dcd047333f6db36c31a2f...  \n",
      "2  93d8f6df1749d08c2ec844a0d3f96b33e18ee2ce4dd4e0...  \n",
      "3  27a33072ab08a07be2eb39e529345fe3ca3a2aca05967b...  \n",
      "4  27a33072ab08a07be2eb39e529345fe3ca3a2aca05967b...  \n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "try:\n",
    "    df = pd.read_csv('/Users/eliduba/Documents/GitHub/edmrec/backend/recommendations/testing/cleaned_for_experiment.csv')  # Replace with your actual file path\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file 'datasets.csv' was not found. Please check the file path and try again.\")\n",
    "    import sys\n",
    "    sys.exit()\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: The file 'datasets.csv' is empty.\")\n",
    "    sys.exit()\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: The file 'datasets.csv' is malformed or contains parsing errors.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine processed title and description for TF-IDF\n",
    "df['combined_text'] = df['title'] + ' ' + df['description']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorization complete.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer with stopword removal\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the combined text\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['combined_text'])\n",
    "\n",
    "# Convert to dense array for similarity computations if needed\n",
    "# (Note: For large datasets, keep it sparse)\n",
    "tfidf_embeddings = tfidf_matrix.toarray()\n",
    "\n",
    "print(\"TF-IDF vectorization complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 indexing complete.\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Prepare documents for BM25 (tokenized)\n",
    "documents = [doc.split() for doc in df['combined_text']]\n",
    "\n",
    "# Initialize BM25\n",
    "bm25 = BM25Okapi(documents)\n",
    "\n",
    "print(\"BM25 indexing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def recommend_datasets(query, top_k=10):\n",
    "    \"\"\"\n",
    "    Generate dataset recommendations based on the input query using BM25 and TF-IDF.\n",
    "    \n",
    "    Parameters:\n",
    "        query (str): The user query.\n",
    "        top_k (int): Number of top recommendations to return.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the recommended datasets with 'id', 'title', 'description', and 'relevance_score'.\n",
    "    \"\"\"\n",
    "  \n",
    "    \n",
    "    # BM25 scores\n",
    "    bm25_scores = bm25.get_scores(query)\n",
    "    \n",
    "    # TF-IDF similarity\n",
    "    query_tfidf = tfidf_vectorizer.transform([query]).toarray()\n",
    "    tfidf_sim = cosine_similarity(query_tfidf, tfidf_embeddings)[0]\n",
    "    \n",
    "    # Normalize BM25 scores and TF-IDF similarities\n",
    "    if np.max(bm25_scores) != 0:\n",
    "        bm25_norm = bm25_scores / np.max(bm25_scores)\n",
    "    else:\n",
    "        bm25_norm = bm25_scores\n",
    "    \n",
    "    if np.max(tfidf_sim) != 0:\n",
    "        tfidf_norm = tfidf_sim / np.max(tfidf_sim)\n",
    "    else:\n",
    "        tfidf_norm = tfidf_sim\n",
    "    \n",
    "    # Combined relevance score (equal weights)\n",
    "    combined_score = (bm25_norm + tfidf_norm) / 2.0\n",
    "    \n",
    "    # Get top_k indices\n",
    "    top_indices = combined_score.argsort()[-top_k:][::-1]\n",
    "    \n",
    "    # Retrieve the top_k datasets\n",
    "    top_datasets = df.iloc[top_indices].copy()\n",
    "    top_datasets['relevance_score'] = combined_score[top_indices]\n",
    "    \n",
    "    return top_datasets[['id', 'title', 'description', 'relevance_score']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                             title  \\\n",
      "146  1134  Avaliações em Português - Amazon e Mercado Livre   \n",
      "42   1013                e-Commerce (Walmart) Sales Dataset   \n",
      "343    14                e-Commerce (Walmart) Sales Dataset   \n",
      "268    13              E-commerce Customer Behavior Dataset   \n",
      "38   1012              E-commerce Customer Behavior Dataset   \n",
      "62   1024                          Sales data of e commerce   \n",
      "67   1031                               Etailers DATA India   \n",
      "424  1519                           Customer_buying_dataset   \n",
      "390  1477                    BigBasket Descriptive Analysis   \n",
      "261  1290                               HPE servers dataset   \n",
      "\n",
      "                                           description  relevance_score  \n",
      "146   Avaliações de produtos da amazon e mercado livre         0.500000  \n",
      "42         Customer Purchase Patterns and Demographics         0.500000  \n",
      "343        Customer Purchase Patterns and Demographics         0.500000  \n",
      "268  Exploring Customer Engagement and Purchasing P...         0.483099  \n",
      "38   Exploring Customer Engagement and Purchasing P...         0.483099  \n",
      "62                           ECOMMERCE SALES DATASHEET         0.455674  \n",
      "67   E-Commerce Sales and Consumer Behavior Analysi...         0.389200  \n",
      "424                     Customer Segmentation Analysis         0.385855  \n",
      "390  A power BI field where I did a Descriptive Dat...         0.359148  \n",
      "261  a dataset fo data analysis from Hewlett Packar...         0.329896  \n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "user_query = \"customer purchase behavior analysis\"\n",
    "\n",
    "# Generate recommendations\n",
    "top_k = 10\n",
    "recommendations = recommend_datasets(user_query, top_k=top_k * 2)\n",
    "unique_recommendations = recommendations.drop_duplicates(subset=['id']).head(top_k)\n",
    "\n",
    "# Display the recommendations\n",
    "print(unique_recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(actual, predicted, k):\n",
    "    \"\"\"\n",
    "    Calculate Precision@K.\n",
    "    \"\"\"\n",
    "    predicted_at_k = predicted[:k]\n",
    "    relevant = set(actual)\n",
    "    recommended = set(predicted_at_k)\n",
    "    return len(relevant & recommended) / k if k > 0 else 0\n",
    "\n",
    "def recall_at_k(actual, predicted, k):\n",
    "    \"\"\"\n",
    "    Calculate Recall@K.\n",
    "    \"\"\"\n",
    "    predicted_at_k = predicted[:k]\n",
    "    relevant = set(actual)\n",
    "    recommended = set(predicted_at_k)\n",
    "    return len(relevant & recommended) / len(relevant) if len(relevant) > 0 else 0\n",
    "\n",
    "def f1_score_at_k(actual, predicted, k):\n",
    "    \"\"\"\n",
    "    Calculate F1 Score@K.\n",
    "    \"\"\"\n",
    "    precision = precision_at_k(actual, predicted, k)\n",
    "    recall = recall_at_k(actual, predicted, k)\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "def average_precision_at_k(actual, predicted, k):\n",
    "    \"\"\"\n",
    "    Compute Average Precision@K.\n",
    "    \"\"\"\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def dcg_at_k(actual, predicted, k):\n",
    "    \"\"\"\n",
    "    Compute Discounted Cumulative Gain@K.\n",
    "    \"\"\"\n",
    "    dcg = 0.0\n",
    "    for i, p in enumerate(predicted[:k]):\n",
    "        if p in actual:\n",
    "            dcg += 1 / np.log2(i + 2)\n",
    "    return dcg\n",
    "\n",
    "def idcg_at_k(actual, k):\n",
    "    \"\"\"\n",
    "    Compute Ideal Discounted Cumulative Gain@K.\n",
    "    \"\"\"\n",
    "    idcg = 0.0\n",
    "    for i in range(min(len(actual), k)):\n",
    "        idcg += 1 / np.log2(i + 2)\n",
    "    return idcg\n",
    "\n",
    "def ndcg_at_k(actual, predicted, k):\n",
    "    \"\"\"\n",
    "    Compute Normalized Discounted Cumulative Gain@K.\n",
    "    \"\"\"\n",
    "    dcg = dcg_at_k(actual, predicted, k)\n",
    "    idcg = idcg_at_k(actual, k)\n",
    "    return dcg / idcg if idcg > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(ground_truth_df, recommendation_func, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate the recommendation system using various metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        ground_truth_df (pd.DataFrame): DataFrame containing 'query' and 'relevant' columns.\n",
    "        recommendation_func (function): Function that takes a query string and returns recommended datasets.\n",
    "        k (int): Number of top recommendations to consider.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Evaluation results with metrics for each query.\n",
    "    \"\"\"\n",
    "    evaluation_results = []\n",
    "    \n",
    "    for index, row in ground_truth_df.iterrows():\n",
    "        query = row['query']\n",
    "        true_relevant_ids = row['relevant']\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommended_datasets = recommendation_func(query, top_k=k)\n",
    "        \n",
    "        if recommended_datasets.empty or 'id' not in recommended_datasets.columns:\n",
    "            print(f\"No recommendations returned for query: {query}\")\n",
    "            predicted_relevant_ids = []\n",
    "        else:\n",
    "            predicted_relevant_ids = recommended_datasets['id'].tolist()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = precision_at_k(true_relevant_ids, predicted_relevant_ids, k)\n",
    "        recall = recall_at_k(true_relevant_ids, predicted_relevant_ids, k)\n",
    "        f1 = f1_score_at_k(true_relevant_ids, predicted_relevant_ids, k)\n",
    "        ap = average_precision_at_k(true_relevant_ids, predicted_relevant_ids, k)\n",
    "        ndcg = ndcg_at_k(true_relevant_ids, predicted_relevant_ids, k)\n",
    "        \n",
    "        # Store the results\n",
    "        evaluation_results.append({\n",
    "            'query': query,\n",
    "            f'precision@{k}': precision,\n",
    "            f'recall@{k}': recall,\n",
    "            f'f1_score@{k}': f1,\n",
    "            f'average_precision@{k}': ap,\n",
    "            f'ndcg@{k}': ndcg\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    evaluation_df = pd.DataFrame(evaluation_results)\n",
    "    \n",
    "    # Display the evaluation results\n",
    "    print(\"Evaluation Results:\")\n",
    "    display(evaluation_df)\n",
    "    \n",
    "    return evaluation_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations_rank_based(ground_truth_df, recommendation_func, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate the recommendation system using Precision@K, Recall@K, F1 Score@K, Average Precision@K, and NDCG@K.\n",
    "    \n",
    "    Parameters:\n",
    "        ground_truth_df (pd.DataFrame): DataFrame containing 'query' and 'relevant' columns.\n",
    "        recommendation_func (function): Function that takes a query string and returns recommended datasets.\n",
    "        k (int): Number of top recommendations to consider.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Evaluation results with metrics for each query.\n",
    "    \"\"\"\n",
    "    evaluation_results = []\n",
    "    \n",
    "    for index, row in ground_truth_df.iterrows():\n",
    "        query = row['query']\n",
    "        true_relevant_ids = row['relevant']\n",
    "        \n",
    "        # Generate recommendations using the provided recommendation function\n",
    "        recommended_datasets = recommendation_func(query, top_k=k)\n",
    "        \n",
    "        # Check if recommended_datasets is not empty and has 'id' column\n",
    "        if recommended_datasets.empty or 'id' not in recommended_datasets.columns:\n",
    "            print(f\"No recommendations returned for query: {query}\")\n",
    "            predicted_relevant_ids = []\n",
    "        else:\n",
    "            # Extract the recommended dataset IDs\n",
    "            predicted_relevant_ids = recommended_datasets['id'].tolist()\n",
    "        \n",
    "        # Calculate all metrics\n",
    "        precision = precision_at_k(true_relevant_ids, predicted_relevant_ids, k)\n",
    "        recall = recall_at_k(true_relevant_ids, predicted_relevant_ids, k)\n",
    "        f1 = f1_score_at_k(true_relevant_ids, predicted_relevant_ids, k)\n",
    "        ap = average_precision_at_k(true_relevant_ids, predicted_relevant_ids, k)\n",
    "        ndcg = ndcg_at_k(true_relevant_ids, predicted_relevant_ids, k)\n",
    "        \n",
    "        # Store the results\n",
    "        evaluation_results.append({\n",
    "            'query': query,\n",
    "            f'precision@{k}': precision,\n",
    "            f'recall@{k}': recall,\n",
    "            f'f1_score@{k}': f1,\n",
    "            f'average_precision@{k}': ap,\n",
    "            f'ndcg@{k}': ndcg\n",
    "        })\n",
    "    \n",
    "    # Convert the evaluation results to a DataFrame\n",
    "    evaluation_df = pd.DataFrame(evaluation_results)\n",
    "    \n",
    "    # Display the evaluation results\n",
    "    print(\"Evaluation Results:\")\n",
    "    display(evaluation_df)\n",
    "    \n",
    "    return evaluation_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the desired value for K\n",
    "# k = 10\n",
    "\n",
    "# # Evaluate the Recommendation System\n",
    "# evaluation_df = evaluate_recommendations(\n",
    "#     ground_truth_df, \n",
    "#     recommend_datasets,  # This is your recommendation function\n",
    "#     k=k\n",
    "# )\n",
    "\n",
    "# # Optionally, save the evaluation results\n",
    "# evaluation_df.to_csv('evaluation_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "\n",
      "First few rows of the dataset:\n",
      "     id                                       title  \\\n",
      "0     1                             E-Commerce Data   \n",
      "1    10  Sales of summer clothes in E-commerce Wish   \n",
      "2  1000         AV JanataHack Cross-Sell Prediction   \n",
      "3  1000                             E-Commerce Data   \n",
      "4  1000                             E-Commerce Data   \n",
      "\n",
      "                                         description  \\\n",
      "0               Actual transactions from UK retailer   \n",
      "1  Top products with ratings and sales performanc...   \n",
      "2                             Janata Hack Cross-Sell   \n",
      "3               Actual transactions from UK retailer   \n",
      "4               Actual transactions from UK retailer   \n",
      "\n",
      "                                                 url   size format  \\\n",
      "0      https://www.kaggle.com/carrie1/ecommerce-data    7MB    csv   \n",
      "1  https://www.kaggle.com/jmmvutu/summer-products...  406KB    csv   \n",
      "2  https://www.kaggle.com/jinxzed/av-janatahack-c...    6MB    csv   \n",
      "3      https://www.kaggle.com/carrie1/ecommerce-data    7MB    csv   \n",
      "4      https://www.kaggle.com/carrie1/ecommerce-data    7MB    csv   \n",
      "\n",
      "                                     tfidf_embedding  \\\n",
      "0  \\x00000000000000000000000000000000000000000000...   \n",
      "1  \\x00000000000000000000000000000000000000000000...   \n",
      "2  \\x00000000000000000000000000000000000000000000...   \n",
      "3  \\x00000000000000000000000000000000000000000000...   \n",
      "4  \\x00000000000000000000000000000000000000000000...   \n",
      "\n",
      "                                      bert_embedding  source  \\\n",
      "0  \\x9c22033d41e9b0bc0ad317bd2b8e56bdcd02cdbb7352...  Kaggle   \n",
      "1  \\xeaf32dbcc6da9f3cbfb747bc71eac73ca4a2713dd6bd...  Kaggle   \n",
      "2  \\x3efc07be92c1a7bd2323ccbcd2855ebcc9c71ebca960...  Kaggle   \n",
      "3  \\x3efc07be92c1a7bd2323ccbcd2855ebcc9c71ebca960...  Kaggle   \n",
      "4  \\x3efc07be92c1a7bd2323ccbcd2855ebcc9c71ebca960...  Kaggle   \n",
      "\n",
      "                   date_added  date_updated  \\\n",
      "0  2024-09-25 22:30:48.753641           NaN   \n",
      "1  2024-09-25 22:31:50.124325           NaN   \n",
      "2  2024-09-25 21:04:47.835632           NaN   \n",
      "3  2024-09-25 23:58:44.849257           NaN   \n",
      "4  2024-09-25 22:57:59.432061           NaN   \n",
      "\n",
      "                                           data_hash  \n",
      "0  27a33072ab08a07be2eb39e529345fe3ca3a2aca05967b...  \n",
      "1  d62eb13482b85e25268a00816dcd047333f6db36c31a2f...  \n",
      "2  93d8f6df1749d08c2ec844a0d3f96b33e18ee2ce4dd4e0...  \n",
      "3  27a33072ab08a07be2eb39e529345fe3ca3a2aca05967b...  \n",
      "4  27a33072ab08a07be2eb39e529345fe3ca3a2aca05967b...  \n",
      "\n",
      "Dataset after removing duplicates:\n",
      "     id                                       title  \\\n",
      "0     1                             E-Commerce Data   \n",
      "1    10  Sales of summer clothes in E-commerce Wish   \n",
      "2  1000         AV JanataHack Cross-Sell Prediction   \n",
      "5  1001          Laptop sales price prediction 2024   \n",
      "8  1002                             E-Commerce Data   \n",
      "\n",
      "                                         description  \\\n",
      "0               Actual transactions from UK retailer   \n",
      "1  Top products with ratings and sales performanc...   \n",
      "2                             Janata Hack Cross-Sell   \n",
      "5  \"Deciphering Laptop Sales: Unveiling Insights ...   \n",
      "8       Sales details from Indian e-commerce website   \n",
      "\n",
      "                                                 url   size format  \\\n",
      "0      https://www.kaggle.com/carrie1/ecommerce-data    7MB    csv   \n",
      "1  https://www.kaggle.com/jmmvutu/summer-products...  406KB    csv   \n",
      "2  https://www.kaggle.com/jinxzed/av-janatahack-c...    6MB    csv   \n",
      "5  https://www.kaggle.com/siddiquifaiznaeem/lapto...   37KB    csv   \n",
      "8    https://www.kaggle.com/benroshan/ecommerce-data   18KB    csv   \n",
      "\n",
      "                                     tfidf_embedding  \\\n",
      "0  \\x00000000000000000000000000000000000000000000...   \n",
      "1  \\x00000000000000000000000000000000000000000000...   \n",
      "2  \\x00000000000000000000000000000000000000000000...   \n",
      "5  \\x00000000000000000000000000000000000000000000...   \n",
      "8  \\x00000000000000000000000000000000000000000000...   \n",
      "\n",
      "                                      bert_embedding  source  \\\n",
      "0  \\x9c22033d41e9b0bc0ad317bd2b8e56bdcd02cdbb7352...  Kaggle   \n",
      "1  \\xeaf32dbcc6da9f3cbfb747bc71eac73ca4a2713dd6bd...  Kaggle   \n",
      "2  \\x3efc07be92c1a7bd2323ccbcd2855ebcc9c71ebca960...  Kaggle   \n",
      "5  \\x719a44bde5c8813c62d3983c86f51fbcbc8d8b3d083c...  Kaggle   \n",
      "8  \\xf4633fbd39b6db3ae0785ebd40c6123d591f253dbdb6...  Kaggle   \n",
      "\n",
      "                   date_added  date_updated  \\\n",
      "0  2024-09-25 22:30:48.753641           NaN   \n",
      "1  2024-09-25 22:31:50.124325           NaN   \n",
      "2  2024-09-25 21:04:47.835632           NaN   \n",
      "5  2024-09-25 21:04:47.945468           NaN   \n",
      "8  2024-09-25 22:57:59.699755           NaN   \n",
      "\n",
      "                                           data_hash  \n",
      "0  27a33072ab08a07be2eb39e529345fe3ca3a2aca05967b...  \n",
      "1  d62eb13482b85e25268a00816dcd047333f6db36c31a2f...  \n",
      "2  93d8f6df1749d08c2ec844a0d3f96b33e18ee2ce4dd4e0...  \n",
      "5  7a85d8f2b5fe93b88c6cc4166e9481746a031f9edd2106...  \n",
      "8  9995a3d9c2620a872407e33e0d631ad7b8ecd668f576da...  \n",
      "\n",
      "Ground Truth DataFrame:\n",
      "                                             query  \\\n",
      "0              customer purchase behavior analysis   \n",
      "1                           product pricing trends   \n",
      "2                inventory management optimization   \n",
      "3              e-commerce website traffic analysis   \n",
      "4                         online sales forecasting   \n",
      "5              customer segmentation in e-commerce   \n",
      "6                  digital marketing effectiveness   \n",
      "7                             product return rates   \n",
      "8                          seasonal sales patterns   \n",
      "9            supply chain management in e-commerce   \n",
      "10                       customer loyalty programs   \n",
      "11          cross-selling strategies in e-commerce   \n",
      "12         social media impact on e-commerce sales   \n",
      "13                      e-commerce fraud detection   \n",
      "14                   multi-channel retail strategy   \n",
      "15            personalized product recommendations   \n",
      "16  user experience design for e-commerce websites   \n",
      "17                       impact of mobile commerce   \n",
      "18       data-driven decision making in e-commerce   \n",
      "19         customer churn prediction in e-commerce   \n",
      "\n",
      "                                             relevant  \n",
      "0   [1025, 1155, 1539, 1541, 1031, 1034, 1290, 12,...  \n",
      "1   [1537, 1028, 1031, 10, 1039, 15, 16, 1046, 104...  \n",
      "2                                        [1209, 1474]  \n",
      "3   [1, 1031, 10, 11, 1036, 12, 1294, 13, 1040, 14...  \n",
      "4   [1024, 1152, 1408, 1027, 1536, 1537, 1542, 103...  \n",
      "5   [1, 1025, 1539, 1400, 1146, 1031, 10, 1034, 11...  \n",
      "6                                              [1306]  \n",
      "7   [1537, 1028, 10, 1039, 15, 16, 1046, 1047, 104...  \n",
      "8   [1024, 1408, 1536, 1027, 1031, 10, 11, 12, 103...  \n",
      "9   [1, 1031, 10, 11, 12, 13, 14, 15, 1040, 16, 10...  \n",
      "10  [1025, 1539, 1034, 13, 14, 16, 1171, 1299, 117...  \n",
      "11  [1, 1031, 10, 11, 12, 13, 14, 15, 1040, 16, 10...  \n",
      "12  [1024, 1, 1408, 1027, 1536, 1031, 10, 11, 12, ...  \n",
      "13  [1, 1283, 1031, 10, 11, 12, 13, 14, 15, 1040, ...  \n",
      "14  [1, 1091, 1123, 1258, 1293, 1006, 1518, 1137, ...  \n",
      "15  [1537, 1028, 10, 1039, 15, 16, 1046, 1047, 104...  \n",
      "16  [1, 1031, 10, 11, 12, 13, 14, 15, 1040, 16, 10...  \n",
      "17                                                 []  \n",
      "18  [1, 1031, 10, 11, 12, 1290, 13, 14, 1040, 1402...  \n",
      "19  [1408, 1, 1031, 1401, 10, 1034, 11, 1037, 12, ...  \n",
      "Warning: No recommendations found for query 'inventory management optimization'.\n",
      "Warning: No recommendations found for query 'e-commerce website traffic analysis'.\n",
      "Warning: No recommendations found for query 'online sales forecasting'.\n",
      "Warning: No recommendations found for query 'customer segmentation in e-commerce'.\n",
      "Warning: No recommendations found for query 'digital marketing effectiveness'.\n",
      "Warning: No recommendations found for query 'product return rates'.\n",
      "Warning: No recommendations found for query 'seasonal sales patterns'.\n",
      "Warning: No recommendations found for query 'supply chain management in e-commerce'.\n",
      "Warning: No recommendations found for query 'customer loyalty programs'.\n",
      "Warning: No recommendations found for query 'cross-selling strategies in e-commerce'.\n",
      "Warning: No recommendations found for query 'social media impact on e-commerce sales'.\n",
      "Warning: No recommendations found for query 'e-commerce fraud detection'.\n",
      "Warning: No recommendations found for query 'multi-channel retail strategy'.\n",
      "Warning: No recommendations found for query 'personalized product recommendations'.\n",
      "Warning: No recommendations found for query 'user experience design for e-commerce websites'.\n",
      "Warning: No recommendations found for query 'impact of mobile commerce'.\n",
      "Warning: No recommendations found for query 'data-driven decision making in e-commerce'.\n",
      "Warning: No recommendations found for query 'customer churn prediction in e-commerce'.\n",
      "\n",
      "Evaluation Results for Other System:\n",
      "                                             query  precision@10  recall@10  \\\n",
      "0              customer purchase behavior analysis           0.1   0.013333   \n",
      "1                           product pricing trends           0.1   0.012346   \n",
      "2                inventory management optimization           0.0   0.000000   \n",
      "3              e-commerce website traffic analysis           0.0   0.000000   \n",
      "4                         online sales forecasting           0.0   0.000000   \n",
      "5              customer segmentation in e-commerce           0.0   0.000000   \n",
      "6                  digital marketing effectiveness           0.0   0.000000   \n",
      "7                             product return rates           0.0   0.000000   \n",
      "8                          seasonal sales patterns           0.0   0.000000   \n",
      "9            supply chain management in e-commerce           0.0   0.000000   \n",
      "10                       customer loyalty programs           0.0   0.000000   \n",
      "11          cross-selling strategies in e-commerce           0.0   0.000000   \n",
      "12         social media impact on e-commerce sales           0.0   0.000000   \n",
      "13                      e-commerce fraud detection           0.0   0.000000   \n",
      "14                   multi-channel retail strategy           0.0   0.000000   \n",
      "15            personalized product recommendations           0.0   0.000000   \n",
      "16  user experience design for e-commerce websites           0.0   0.000000   \n",
      "17                       impact of mobile commerce           0.0   0.000000   \n",
      "18       data-driven decision making in e-commerce           0.0   0.000000   \n",
      "19         customer churn prediction in e-commerce           0.0   0.000000   \n",
      "\n",
      "    f1_score@10  average_precision@10   ndcg@10  \n",
      "0      0.023529              0.014286  0.073364  \n",
      "1      0.021978              0.100000  0.220092  \n",
      "2      0.000000              0.000000  0.000000  \n",
      "3      0.000000              0.000000  0.000000  \n",
      "4      0.000000              0.000000  0.000000  \n",
      "5      0.000000              0.000000  0.000000  \n",
      "6      0.000000              0.000000  0.000000  \n",
      "7      0.000000              0.000000  0.000000  \n",
      "8      0.000000              0.000000  0.000000  \n",
      "9      0.000000              0.000000  0.000000  \n",
      "10     0.000000              0.000000  0.000000  \n",
      "11     0.000000              0.000000  0.000000  \n",
      "12     0.000000              0.000000  0.000000  \n",
      "13     0.000000              0.000000  0.000000  \n",
      "14     0.000000              0.000000  0.000000  \n",
      "15     0.000000              0.000000  0.000000  \n",
      "16     0.000000              0.000000  0.000000  \n",
      "17     0.000000              0.000000  0.000000  \n",
      "18     0.000000              0.000000  0.000000  \n",
      "19     0.000000              0.000000  0.000000  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not convert ['customer purchase behavior analysisproduct pricing trendsinventory management optimizatione-commerce website traffic analysisonline sales forecastingcustomer segmentation in e-commercedigital marketing effectivenessproduct return ratesseasonal sales patternssupply chain management in e-commercecustomer loyalty programscross-selling strategies in e-commercesocial media impact on e-commerce salese-commerce fraud detectionmulti-channel retail strategypersonalized product recommendationsuser experience design for e-commerce websitesimpact of mobile commercedata-driven decision making in e-commercecustomer churn prediction in e-commerce'] to numeric",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 388\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28mprint\u001b[39m(evaluation_df_other_system)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Aggregate Metrics\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# Calculate average metrics across all queries\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m aggregated_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_df_other_system\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_frame()\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    389\u001b[0m aggregated_metrics\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOther System\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAggregated Evaluation Metrics for Other System:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/trehaus-backend/lib/python3.12/site-packages/pandas/core/frame.py:11693\u001b[0m, in \u001b[0;36mDataFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11685\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m  11686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  11687\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11691\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11692\u001b[0m ):\n\u001b[0;32m> 11693\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  11694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series):\n\u001b[1;32m  11695\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/trehaus-backend/lib/python3.12/site-packages/pandas/core/generic.py:12420\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  12414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  12415\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12418\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  12419\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 12420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12421\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m  12422\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/trehaus-backend/lib/python3.12/site-packages/pandas/core/generic.py:12377\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12373\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_func(name, (), kwargs)\n\u001b[1;32m  12375\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m> 12377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[1;32m  12379\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/trehaus-backend/lib/python3.12/site-packages/pandas/core/frame.py:11562\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m  11558\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m  11560\u001b[0m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[1;32m  11561\u001b[0m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[0;32m> 11562\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  11563\u001b[0m out \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(res, axes\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m  11564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboolean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/trehaus-backend/lib/python3.12/site-packages/pandas/core/internals/managers.py:1500\u001b[0m, in \u001b[0;36mBlockManager.reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1498\u001b[0m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m-> 1500\u001b[0m     nbs \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m     res_blocks\u001b[38;5;241m.\u001b[39mextend(nbs)\n\u001b[1;32m   1503\u001b[0m index \u001b[38;5;241m=\u001b[39m Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/trehaus-backend/lib/python3.12/site-packages/pandas/core/internals/blocks.py:404\u001b[0m, in \u001b[0;36mBlock.reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 404\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    407\u001b[0m         res_values \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m~/.pyenv/versions/trehaus-backend/lib/python3.12/site-packages/pandas/core/frame.py:11481\u001b[0m, in \u001b[0;36mDataFrame._reduce.<locals>.blk_func\u001b[0;34m(values, axis)\u001b[0m\n\u001b[1;32m  11479\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([result])\n\u001b[1;32m  11480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m> 11481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/trehaus-backend/lib/python3.12/site-packages/pandas/core/nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    145\u001b[0m         result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/trehaus-backend/lib/python3.12/site-packages/pandas/core/nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[0;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _wrap_results(result, orig_values\u001b[38;5;241m.\u001b[39mdtype, fill_value\u001b[38;5;241m=\u001b[39miNaT)\n",
      "File \u001b[0;32m~/.pyenv/versions/trehaus-backend/lib/python3.12/site-packages/pandas/core/nanops.py:720\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    718\u001b[0m count \u001b[38;5;241m=\u001b[39m _get_counts(values\u001b[38;5;241m.\u001b[39mshape, mask, axis, dtype\u001b[38;5;241m=\u001b[39mdtype_count)\n\u001b[1;32m    719\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39msum(axis, dtype\u001b[38;5;241m=\u001b[39mdtype_sum)\n\u001b[0;32m--> 720\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthe_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    723\u001b[0m     count \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, count)\n",
      "File \u001b[0;32m~/.pyenv/versions/trehaus-backend/lib/python3.12/site-packages/pandas/core/nanops.py:1686\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1683\u001b[0m inferred \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39minfer_dtype(x)\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inferred \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;66;03m# GH#44008, GH#36703 avoid casting e.g. strings to numeric\u001b[39;00m\n\u001b[0;32m-> 1686\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to numeric\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1688\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mcomplex128)\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not convert ['customer purchase behavior analysisproduct pricing trendsinventory management optimizatione-commerce website traffic analysisonline sales forecastingcustomer segmentation in e-commercedigital marketing effectivenessproduct return ratesseasonal sales patternssupply chain management in e-commercecustomer loyalty programscross-selling strategies in e-commercesocial media impact on e-commerce salese-commerce fraud detectionmulti-channel retail strategypersonalized product recommendationsuser experience design for e-commerce websitesimpact of mobile commercedata-driven decision making in e-commercecustomer churn prediction in e-commerce'] to numeric"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Import Necessary Libraries\n",
    "# ============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================\n",
    "# Load and Clean the Dataset\n",
    "# ============================\n",
    "\n",
    "# Path to your dataset CSV file\n",
    "DATASET_PATH = '/Users/eliduba/Documents/GitHub/edmrec/backend/recommendations/testing/cleaned_for_experiment.csv'  # Replace with your actual file path\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(DATASET_PATH)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{DATASET_PATH}' was not found.\")\n",
    "    exit()\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(f\"Error: The file '{DATASET_PATH}' is empty.\")\n",
    "    exit()\n",
    "except pd.errors.ParserError:\n",
    "    print(f\"Error: The file '{DATASET_PATH}' is malformed.\")\n",
    "    exit()\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Handle missing data by filling with empty strings\n",
    "df['title'] = df['title'].fillna('')\n",
    "df['description'] = df['description'].fillna('')\n",
    "\n",
    "# Remove duplicates based on 'id' to ensure uniqueness\n",
    "df = df.drop_duplicates(subset=['id'], keep='first').copy()\n",
    "print(\"\\nDataset after removing duplicates:\")\n",
    "print(df.head())\n",
    "\n",
    "# ============================\n",
    "# Define Queries and Associated Keywords\n",
    "# ============================\n",
    "\n",
    "# Define the queries and associated keywords\n",
    "queries = {\n",
    "    'customer purchase behavior analysis': ['customer', 'purchase', 'behavior', 'analysis', 'buying patterns', 'consumer behavior'],\n",
    "    'product pricing trends': ['product', 'pricing', 'price trends', 'cost analysis', 'market trends', 'price changes'],\n",
    "    'inventory management optimization': ['inventory', 'management', 'optimization', 'stock control', 'supply management', 'inventory levels'],\n",
    "    'e-commerce website traffic analysis': ['e-commerce', 'website', 'traffic', 'site visitors', 'web analytics', 'user traffic'],\n",
    "    'online sales forecasting': ['online', 'sales', 'forecasting', 'prediction', 'future sales', 'sales trends'],\n",
    "    'customer segmentation in e-commerce': ['customer', 'segmentation', 'e-commerce', 'grouping', 'market segments', 'consumer categories'],\n",
    "    'digital marketing effectiveness': ['digital marketing', 'effectiveness', 'ROI', 'campaign performance', 'online advertising', 'marketing impact'],\n",
    "    'product return rates': ['product', 'return rates', 'returns', 'customer returns', 'return analysis', 'refunds'],\n",
    "    'seasonal sales patterns': ['seasonal', 'sales', 'patterns', 'seasonal trends', 'holiday sales', 'time-based sales'],\n",
    "    'supply chain management in e-commerce': ['supply chain', 'management', 'e-commerce', 'logistics', 'distribution', 'supply network'],\n",
    "    'customer loyalty programs': ['customer', 'loyalty', 'programs', 'loyalty rewards', 'customer retention', 'loyalty schemes'],\n",
    "    'cross-selling strategies in e-commerce': ['cross-selling', 'strategies', 'e-commerce', 'upselling', 'additional sales', 'related products'],\n",
    "    'social media impact on e-commerce sales': ['social media', 'impact', 'e-commerce', 'sales', 'influence', 'social commerce'],\n",
    "    'e-commerce fraud detection': ['e-commerce', 'fraud', 'detection', 'fraud prevention', 'scam', 'online fraud'],\n",
    "    'multi-channel retail strategy': ['multi-channel', 'retail', 'strategy', 'omnichannel', 'sales channels', 'customer touchpoints'],\n",
    "    'personalized product recommendations': ['personalized', 'product', 'recommendations', 'customized', 'recommendation engine', 'suggested products'],\n",
    "    'user experience design for e-commerce websites': ['user experience', 'design', 'e-commerce', 'websites', 'UX', 'interface design'],\n",
    "    'impact of mobile commerce': ['impact', 'mobile commerce', 'm-commerce', 'mobile sales', 'smartphone shopping', 'mobile influence'],\n",
    "    'data-driven decision making in e-commerce': ['data-driven', 'decision making', 'e-commerce', 'analytics', 'data analysis', 'business intelligence'],\n",
    "    'customer churn prediction in e-commerce': ['customer churn', 'prediction', 'e-commerce', 'attrition', 'churn analysis', 'customer loss'],\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Create Ground Truth Data\n",
    "# ============================\n",
    "\n",
    "# Create an empty list to store the ground truth\n",
    "ground_truth = []\n",
    "\n",
    "# Function to preprocess text (consistent with embeddings)\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess text by lowercasing. Extend this function as needed.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): Text to preprocess.\n",
    "    \n",
    "    Returns:\n",
    "        str: Preprocessed text.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "# Iterate over each query and its associated keywords to build ground truth\n",
    "for query, keywords in queries.items():\n",
    "    relevant_datasets = []\n",
    "    \n",
    "    # Iterate through all datasets to find relevant ones\n",
    "    for idx, row in df.iterrows():\n",
    "        title = row['title']\n",
    "        description = row['description']\n",
    "        dataset_id = row['id']\n",
    "        \n",
    "        # Preprocess title and description\n",
    "        title_lower = preprocess_text(title)\n",
    "        description_lower = preprocess_text(description)\n",
    "        \n",
    "        # Check if any keyword is in title or description\n",
    "        if any(keyword.lower() in title_lower or keyword.lower() in description_lower for keyword in keywords):\n",
    "            relevant_datasets.append(dataset_id)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    relevant_datasets = list(set(relevant_datasets))\n",
    "    \n",
    "    # Append to ground truth\n",
    "    ground_truth.append({\n",
    "        'query': query,\n",
    "        'relevant': relevant_datasets\n",
    "    })\n",
    "\n",
    "# Convert ground truth to DataFrame\n",
    "ground_truth_df = pd.DataFrame(ground_truth)\n",
    "\n",
    "# Display the ground truth DataFrame\n",
    "print(\"\\nGround Truth DataFrame:\")\n",
    "print(ground_truth_df)\n",
    "\n",
    "# ============================\n",
    "# Load or Define Other System's Recommendations\n",
    "# ============================\n",
    "\n",
    "# Example: Load recommendations from a CSV or define them manually\n",
    "# For demonstration, we'll define them manually as per your initial example\n",
    "# Replace this with your actual other system's recommendations\n",
    "\n",
    "other_system_recommendations = {\n",
    "    'customer purchase behavior analysis': pd.DataFrame({\n",
    "        'id': [305, 270, 304, 386, 857, 438, 1519, 286, 999, 888],\n",
    "        'title': [\n",
    "            \"🛒 E-commerce Customer Data For Behavior Analysis\",\n",
    "            \"Customer Purchases Behaviour Dataset\",\n",
    "            \"E-commerce Customer Behavior Dataset\",\n",
    "            \"Digital Marketing | E-Commerce | Customer Behavior\",\n",
    "            \"E commerce product purchase data\",\n",
    "            \"E-Commerce Customer Dataset\",\n",
    "            \"Customer_buying_dataset\",\n",
    "            \"E-commerce Customer Behavior Dataset\",\n",
    "            \"Additional Dataset 1\",\n",
    "            \"Additional Dataset 2\"\n",
    "        ],\n",
    "        'description': [\n",
    "            \"Explore Customer Shopping Habits, Churn, and Purchase Patterns.\",\n",
    "            \"Simulated Dataset of Customer Purchase Behavior.\",\n",
    "            \"Synthetic Customer Behavior Dataset for E-commerce Analysis.\",\n",
    "            \"Customer behavior data on e-commerce for churn analysis.\",\n",
    "            \"Detailed product purchase data from various e-commerce platforms.\",\n",
    "            \"E-Commerce Customer Purchase and Interaction Data.\",\n",
    "            \"Customer Segmentation and Buying Patterns Analysis.\",\n",
    "            \"Exploring Customer Engagement and Purchasing Patterns.\",\n",
    "            \"Extra dataset description 1.\",\n",
    "            \"Extra dataset description 2.\"\n",
    "        ],\n",
    "        'relevance_score': [0.655477, 0.639732, 0.616212, 0.561961, 0.523510, 0.498224, 0.489399, 0.468744, 0.450000, 0.440000]\n",
    "    }),\n",
    "    'product pricing trends': pd.DataFrame({\n",
    "        'id': [1013, 14, 13, 1012, 1024],\n",
    "        'title': [\n",
    "            \"e-Commerce (Walmart) Sales Dataset\",\n",
    "            \"e-Commerce (Walmart) Sales Dataset\",\n",
    "            \"E-commerce Customer Behavior Dataset\",\n",
    "            \"E-commerce Customer Behavior Dataset\",\n",
    "            \"Sales data of e commerce\"\n",
    "        ],\n",
    "        'description': [\n",
    "            \"Customer Purchase Patterns and Demographics.\",\n",
    "            \"Customer Purchase Patterns and Demographics.\",\n",
    "            \"Exploring Customer Engagement and Purchasing Patterns.\",\n",
    "            \"Exploring Customer Engagement and Purchasing Patterns.\",\n",
    "            \"ECOMMERCE SALES DATASHEET\"\n",
    "        ],\n",
    "        'relevance_score': [0.500000, 0.500000, 0.483099, 0.483099, 0.455674]\n",
    "    }),\n",
    "    # Add more query recommendations as needed\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Define Evaluation Metrics\n",
    "# ============================\n",
    "\n",
    "def precision_at_k(actual, predicted, k):\n",
    "    \"\"\"\n",
    "    Calculate Precision@K.\n",
    "    \n",
    "    Parameters:\n",
    "        actual (list): List of relevant dataset IDs.\n",
    "        predicted (list): List of recommended dataset IDs.\n",
    "        k (int): Number of top recommendations to consider.\n",
    "    \n",
    "    Returns:\n",
    "        float: Precision@K score.\n",
    "    \"\"\"\n",
    "    predicted_at_k = predicted[:k]\n",
    "    relevant = set(actual)\n",
    "    recommended = set(predicted_at_k)\n",
    "    return len(relevant & recommended) / k if k > 0 else 0\n",
    "\n",
    "def recall_at_k(actual, predicted, k):\n",
    "    \"\"\"\n",
    "    Calculate Recall@K.\n",
    "    \n",
    "    Parameters:\n",
    "        actual (list): List of relevant dataset IDs.\n",
    "        predicted (list): List of recommended dataset IDs.\n",
    "        k (int): Number of top recommendations to consider.\n",
    "    \n",
    "    Returns:\n",
    "        float: Recall@K score.\n",
    "    \"\"\"\n",
    "    predicted_at_k = predicted[:k]\n",
    "    relevant = set(actual)\n",
    "    recommended = set(predicted_at_k)\n",
    "    return len(relevant & recommended) / len(relevant) if len(relevant) > 0 else 0\n",
    "\n",
    "def f1_score_at_k(actual, predicted, k):\n",
    "    \"\"\"\n",
    "    Calculate F1 Score@K.\n",
    "    \n",
    "    Parameters:\n",
    "        actual (list): List of relevant dataset IDs.\n",
    "        predicted (list): List of recommended dataset IDs.\n",
    "        k (int): Number of top recommendations to consider.\n",
    "    \n",
    "    Returns:\n",
    "        float: F1 Score@K.\n",
    "    \"\"\"\n",
    "    precision = precision_at_k(actual, predicted, k)\n",
    "    recall = recall_at_k(actual, predicted, k)\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "def average_precision_at_k(actual, predicted, k):\n",
    "    \"\"\"\n",
    "    Compute Average Precision@K.\n",
    "    \n",
    "    Parameters:\n",
    "        actual (list): List of relevant dataset IDs.\n",
    "        predicted (list): List of recommended dataset IDs.\n",
    "        k (int): Number of top recommendations to consider.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average Precision@K.\n",
    "    \"\"\"\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def dcg_at_k(actual, predicted, k):\n",
    "    \"\"\"\n",
    "    Compute Discounted Cumulative Gain@K.\n",
    "    \n",
    "    Parameters:\n",
    "        actual (list): List of relevant dataset IDs.\n",
    "        predicted (list): List of recommended dataset IDs.\n",
    "        k (int): Number of top recommendations to consider.\n",
    "    \n",
    "    Returns:\n",
    "        float: DCG@K.\n",
    "    \"\"\"\n",
    "    dcg = 0.0\n",
    "    for i, p in enumerate(predicted[:k]):\n",
    "        if p in actual:\n",
    "            dcg += 1 / np.log2(i + 2)\n",
    "    return dcg\n",
    "\n",
    "def idcg_at_k(actual, k):\n",
    "    \"\"\"\n",
    "    Compute Ideal Discounted Cumulative Gain@K.\n",
    "    \n",
    "    Parameters:\n",
    "        actual (list): List of relevant dataset IDs.\n",
    "        k (int): Number of top recommendations to consider.\n",
    "    \n",
    "    Returns:\n",
    "        float: IDCG@K.\n",
    "    \"\"\"\n",
    "    idcg = 0.0\n",
    "    for i in range(min(len(actual), k)):\n",
    "        idcg += 1 / np.log2(i + 2)\n",
    "    return idcg\n",
    "\n",
    "def ndcg_at_k(actual, predicted, k):\n",
    "    \"\"\"\n",
    "    Compute Normalized Discounted Cumulative Gain@K.\n",
    "    \n",
    "    Parameters:\n",
    "        actual (list): List of relevant dataset IDs.\n",
    "        predicted (list): List of recommended dataset IDs.\n",
    "        k (int): Number of top recommendations to consider.\n",
    "    \n",
    "    Returns:\n",
    "        float: NDCG@K.\n",
    "    \"\"\"\n",
    "    dcg = dcg_at_k(actual, predicted, k)\n",
    "    idcg = idcg_at_k(actual, k)\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "# ============================\n",
    "# Evaluate the Other Recommendation System\n",
    "# ============================\n",
    "\n",
    "def evaluate_other_system(ground_truth_df, other_system_recs, top_k=10):\n",
    "    \"\"\"\n",
    "    Evaluate the other recommendation system against the ground truth.\n",
    "    \n",
    "    Parameters:\n",
    "        ground_truth_df (pd.DataFrame): DataFrame with 'query' and 'relevant' columns.\n",
    "        other_system_recs (dict): Dictionary with queries as keys and DataFrames of recommendations as values.\n",
    "        top_k (int): Number of top recommendations to consider.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Evaluation metrics for each query.\n",
    "    \"\"\"\n",
    "    evaluation_results = []\n",
    "    \n",
    "    for index, row in ground_truth_df.iterrows():\n",
    "        query = row['query']\n",
    "        true_relevant = row['relevant']\n",
    "        \n",
    "        # Get recommendations for the query\n",
    "        if query in other_system_recs:\n",
    "            recs_df = other_system_recs[query]\n",
    "            recommended_ids = recs_df['id'].tolist()\n",
    "        else:\n",
    "            print(f\"Warning: No recommendations found for query '{query}'.\")\n",
    "            recommended_ids = []\n",
    "        \n",
    "        # Calculate metrics\n",
    "        prec = precision_at_k(true_relevant, recommended_ids, top_k)\n",
    "        rec = recall_at_k(true_relevant, recommended_ids, top_k)\n",
    "        f1 = f1_score_at_k(true_relevant, recommended_ids, top_k)\n",
    "        ap = average_precision_at_k(true_relevant, recommended_ids, top_k)\n",
    "        ndcg = ndcg_at_k(true_relevant, recommended_ids, top_k)\n",
    "        \n",
    "        # Append results\n",
    "        evaluation_results.append({\n",
    "            'query': query,\n",
    "            'precision@{}'.format(top_k): prec,\n",
    "            'recall@{}'.format(top_k): rec,\n",
    "            'f1_score@{}'.format(top_k): f1,\n",
    "            'average_precision@{}'.format(top_k): ap,\n",
    "            'ndcg@{}'.format(top_k): ndcg\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    evaluation_df = pd.DataFrame(evaluation_results)\n",
    "    \n",
    "    return evaluation_df\n",
    "\n",
    "# ============================\n",
    "# Run the Evaluation\n",
    "# ============================\n",
    "\n",
    "# Perform evaluation\n",
    "evaluation_df_other_system = evaluate_other_system(\n",
    "    ground_truth_df=ground_truth_df,\n",
    "    other_system_recs=other_system_recommendations,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "# Display Evaluation Results\n",
    "print(\"\\nEvaluation Results for Other System:\")\n",
    "print(evaluation_df_other_system)\n",
    "\n",
    "# ============================\n",
    "# Aggregate Metrics\n",
    "# ============================\n",
    "\n",
    "# # Calculate average metrics across all queries\n",
    "# aggregated_metrics = evaluation_df_other_system.mean().to_frame().T\n",
    "# aggregated_metrics.insert(0, 'system', 'Other System')\n",
    "\n",
    "# print(\"\\nAggregated Evaluation Metrics for Other System:\")\n",
    "# print(aggregated_metrics)\n",
    "\n",
    "# # ============================\n",
    "# # Visualize the Evaluation Metrics\n",
    "# # ============================\n",
    "\n",
    "# def visualize_metrics(evaluation_df, aggregated_metrics, system_name='Other System'):\n",
    "#     \"\"\"\n",
    "#     Visualize the evaluation metrics.\n",
    "    \n",
    "#     Parameters:\n",
    "#         evaluation_df (pd.DataFrame): DataFrame with per-query evaluation metrics.\n",
    "#         aggregated_metrics (pd.DataFrame): DataFrame with aggregated metrics.\n",
    "#         system_name (str): Name of the system being visualized.\n",
    "#     \"\"\"\n",
    "#     # Melt the evaluation_df for easier plotting\n",
    "#     melted_df = evaluation_df.melt(id_vars=['query'], \n",
    "#                                    value_vars=['precision@10', 'recall@10', 'f1_score@10', 'average_precision@10', 'ndcg@10'],\n",
    "#                                    var_name='metric', \n",
    "#                                    value_name='score')\n",
    "    \n",
    "#     # Set plot style\n",
    "#     sns.set(style=\"whitegrid\")\n",
    "    \n",
    "#     # Create a bar plot for per-query metrics\n",
    "#     plt.figure(figsize=(14, 8))\n",
    "#     sns.barplot(x='metric', y='score', data=melted_df, palette='viridis')\n",
    "#     plt.title(f'Evaluation Metrics for {system_name}')\n",
    "#     plt.xlabel('Metric')\n",
    "#     plt.ylabel('Score')\n",
    "#     plt.ylim(0, 1)\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Create a bar plot for aggregated metrics\n",
    "#     melted_agg = aggregated_metrics.melt(id_vars=['system'], \n",
    "#                                          value_vars=['precision@10', 'recall@10', 'f1_score@10', 'average_precision@10', 'ndcg@10'],\n",
    "#                                          var_name='metric', \n",
    "#                                          value_name='score')\n",
    "    \n",
    "#     plt.figure(figsize=(14, 8))\n",
    "#     sns.barplot(x='metric', y='score', data=melted_agg, palette='magma')\n",
    "#     plt.title(f'Aggregated Evaluation Metrics for {system_name}')\n",
    "#     plt.xlabel('Metric')\n",
    "#     plt.ylabel('Average Score')\n",
    "#     plt.ylim(0, 1)\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.show()\n",
    "\n",
    "# # Visualize the metrics for the other system\n",
    "# visualize_metrics(evaluation_df_other_system, aggregated_metrics, system_name='Other System')\n",
    "\n",
    "# ============================\n",
    "# Optional: Statistical Significance Testing\n",
    "# ============================\n",
    "\n",
    "# If you have evaluation metrics from your system, you can perform a paired t-test to determine if the differences are statistically significant.\n",
    "# Here, we'll outline the steps, but you'll need to provide your system's evaluation DataFrame.\n",
    "\n",
    "# Example:\n",
    "# Suppose you have your system's evaluation metrics in `evaluation_df_your_system`\n",
    "# Ensure that both DataFrames have the same queries in the same order.\n",
    "\n",
    "# Uncomment and modify the following code as needed.\n",
    "\n",
    "# # Example: Define your system's evaluation DataFrame\n",
    "# evaluation_df_your_system = pd.DataFrame([\n",
    "#     {'query': 'customer purchase behavior analysis', 'precision@10': 0.80, 'recall@10': 0.60, 'f1_score@10': 0.69, 'average_precision@10': 0.70, 'ndcg@10': 0.75},\n",
    "#     {'query': 'product pricing trends', 'precision@10': 0.65, 'recall@10': 0.50, 'f1_score@10': 0.57, 'average_precision@10': 0.60, 'ndcg@10': 0.65},\n",
    "#     # Add more queries\n",
    "# ])\n",
    "\n",
    "# # Merge the two evaluation DataFrames on 'query'\n",
    "# merged_df = pd.merge(\n",
    "#     evaluation_df_your_system[['query', 'f1_score@10']],\n",
    "#     evaluation_df_other_system[['query', 'f1_score@10']],\n",
    "#     on='query',\n",
    "#     suffixes=('_your_system', '_other_system')\n",
    "# )\n",
    "\n",
    "# # Perform paired t-test on F1 scores\n",
    "# t_stat, p_value = ttest_rel(merged_df['f1_score@10_your_system'], merged_df['f1_score@10_other_system'])\n",
    "\n",
    "# print(f\"\\nPaired t-test results: t-statistic = {t_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "\n",
    "# if p_value < 0.05:\n",
    "#     print(\"The difference in F1 scores is statistically significant.\")\n",
    "# else:\n",
    "#     print(\"The difference in F1 scores is not statistically significant.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trehaus-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
